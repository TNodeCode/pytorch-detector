{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc0f6b5-cb5a-4dfb-9803-1353fbc1332d",
   "metadata": {},
   "source": [
    "# Faster-RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2e361f-65c4-4aef-9d0a-f3d2740e11e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\tilof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00511c2-2d63-41ad-a074-b406be7852d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_filename(epoch: int):\n",
    "    return f\"faster_rcnn_epoch{str(epoch).zfill(4)}.pth\"\n",
    "\n",
    "def build_model(num_classes: int, resume: str, device: str = \"cpu\"):\n",
    "    # Instantiate model with default weights\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    # Replace the prdiction head\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(model.roi_heads.box_predictor.cls_score.in_features, num_classes+1)\n",
    "    # Load custom weights\n",
    "    if resume:\n",
    "        model.load_state_dict(torch.load(resume))\n",
    "    return model.to(device)\n",
    "\n",
    "def build_coco_dataset(root: str, annFile: str, transform):\n",
    "    return CocoDetection(root=root, annFile=annFile, transform=transform)\n",
    "\n",
    "def build_dataloader(dataset, batch_size: int, collate_fn=None):\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14b7e2d1-c7e6-4cb0-aa73-39479597ff17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 300\n",
    "start_epoch = 260\n",
    "num_classes = 1\n",
    "lr = 1e-4\n",
    "batch_size = 32\n",
    "save_every = 10\n",
    "if start_epoch and start_epoch > 0:\n",
    "    resume = get_weight_filename(start_epoch)\n",
    "else:\n",
    "    resume = False\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7f8c2-88d2-4cfb-9ce2-b502c68de8e1",
   "metadata": {},
   "source": [
    "```python\n",
    "# Generate random data, 4 images and 11 bboxes per image with 4 coordinates\n",
    "images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)\n",
    "\n",
    "# Add x0 to x1 and y0 to y1, so that x1 cannot be smaller than x0 and same for y1 and y0\n",
    "boxes[:, :, 2:4] = boxes[:, :, 0:2] + boxes[:, :, 2:4]\n",
    "\n",
    "# Generate random labels (COCO has 91 classes)\n",
    "labels = torch.randint(1, 91, (4, 11))\n",
    "\n",
    "images = list(image for image in images)\n",
    "targets = []\n",
    "\n",
    "for i in range(len(images)):\n",
    "    d = {}\n",
    "    d['boxes'] = boxes[i]\n",
    "    d['labels'] = labels[i]\n",
    "    targets.append(d)\n",
    "output = model(images, targets)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b8c457-c7bb-449f-b123-74924242d5b5",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f04d89a6-7e3a-44e4-95c5-7d77150c4ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tilof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "# Dataset\n",
    "root_dir = \"C:\\\\Users\\\\tilof\\\\PycharmProjects\\\\DeepLearningProjects\\\\DETR\\\\data\\\\spine\"\n",
    "train_data_dir = f\"{root_dir}\\\\train2017\"\n",
    "train_annotation_file = f\"{root_dir}\\\\annotations\\\\instances_train2017.json\"\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.ToDtype(torch.float32),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def collate_fn(data):\n",
    "    return data\n",
    "\n",
    "train_dataset = CocoDetection(root=train_data_dir, annFile=train_annotation_file, transform=transforms)\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Training parameters\n",
    "optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=3, gamma=0.1)\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(model.roi_heads.box_predictor.cls_score.in_features, num_classes+1)\n",
    "if resume:\n",
    "    model.load_state_dict(torch.load(resume))\n",
    "model.to(device)\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4080e2f1-aa70-4bbb-9536-9b8f66fb8784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "Epoch 261/300: loss=2.5225654058158398, loss_classifier=0.9256681092083454, loss_objectness=0.04960546304937452, loss_rpn_box_reg=0.19005899969488382\n",
      "Epoch 262/300: loss=2.3891897723078728, loss_classifier=0.9113056752830744, loss_objectness=0.05529198460862972, loss_rpn_box_reg=0.1851542186923325\n",
      "Epoch 263/300: loss=2.3634716384112835, loss_classifier=0.9206626173108816, loss_objectness=0.0515775510284584, loss_rpn_box_reg=0.1854521429631859\n",
      "Epoch 264/300: loss=2.313635878264904, loss_classifier=0.8984736483544111, loss_objectness=0.05289038963383064, loss_rpn_box_reg=0.1828618124127388\n",
      "Epoch 265/300: loss=2.3127665668725967, loss_classifier=0.9095406178385019, loss_objectness=0.049054142844397575, loss_rpn_box_reg=0.18446182762272656\n",
      "Epoch 266/300: loss=2.2980101443827152, loss_classifier=0.9081164114177227, loss_objectness=0.05372771347174421, loss_rpn_box_reg=0.1831319483462721\n",
      "Epoch 267/300: loss=2.2739234678447247, loss_classifier=0.8927338849753141, loss_objectness=0.05023377580801025, loss_rpn_box_reg=0.18246602499857545\n",
      "Epoch 268/300: loss=2.276223249733448, loss_classifier=0.89531110227108, loss_objectness=0.05123732320498675, loss_rpn_box_reg=0.18273248220793903\n",
      "Epoch 269/300: loss=2.2663408890366554, loss_classifier=0.8996168375015259, loss_objectness=0.046104072651360184, loss_rpn_box_reg=0.1821617262903601\n",
      "Epoch 270/300: loss=2.265767674893141, loss_classifier=0.8994755782186985, loss_objectness=0.0518816971743945, loss_rpn_box_reg=0.18061781185679138\n",
      "Epoch 271/300: loss=2.2574925981462, loss_classifier=0.8989988807588816, loss_objectness=0.04872739710845053, loss_rpn_box_reg=0.1818474279716611\n",
      "Epoch 272/300: loss=2.2473264075815678, loss_classifier=0.8992726784199476, loss_objectness=0.04261143924668431, loss_rpn_box_reg=0.18171021342277527\n",
      "Epoch 273/300: loss=2.2491829469799995, loss_classifier=0.8956894967705011, loss_objectness=0.0472031683602836, loss_rpn_box_reg=0.18149193050339818\n",
      "Epoch 274/300: loss=2.2460655122995377, loss_classifier=0.8949571084231138, loss_objectness=0.04973246980807744, loss_rpn_box_reg=0.18216278194449842\n",
      "Epoch 275/300: loss=2.2407611422240734, loss_classifier=0.8987230286002159, loss_objectness=0.04545331379631534, loss_rpn_box_reg=0.18177366023883224\n",
      "Epoch 276/300: loss=2.242851994931698, loss_classifier=0.8954101148992777, loss_objectness=0.054187764224479906, loss_rpn_box_reg=0.18057458684779704\n",
      "Epoch 277/300: loss=2.234829396009445, loss_classifier=0.8912298213690519, loss_objectness=0.05210224442998879, loss_rpn_box_reg=0.18115708976984024\n",
      "Epoch 278/300: loss=2.219251047819853, loss_classifier=0.8775785453617573, loss_objectness=0.05243774221162312, loss_rpn_box_reg=0.18249782850034535\n",
      "Epoch 279/300: loss=2.2184162698686123, loss_classifier=0.8852304238826036, loss_objectness=0.05056252298527397, loss_rpn_box_reg=0.18154312716796994\n",
      "Epoch 280/300: loss=2.2151412181556225, loss_classifier=0.8834309726953506, loss_objectness=0.05025428780936636, loss_rpn_box_reg=0.1813918063417077\n",
      "Epoch 281/300: loss=2.2172398045659065, loss_classifier=0.8876388538628817, loss_objectness=0.046086972375633195, loss_rpn_box_reg=0.18143977248109877\n",
      "Epoch 282/300: loss=2.231119252741337, loss_classifier=0.8870111778378487, loss_objectness=0.05171330407029018, loss_rpn_box_reg=0.18175675650127232\n",
      "Epoch 283/300: loss=2.2032028548419476, loss_classifier=0.8701299894601107, loss_objectness=0.05533996218582615, loss_rpn_box_reg=0.18152491585351527\n",
      "Epoch 284/300: loss=2.227162826806307, loss_classifier=0.8997509889304638, loss_objectness=0.049781872890889645, loss_rpn_box_reg=0.18094768631272018\n",
      "Epoch 285/300: loss=2.201258212327957, loss_classifier=0.874529629945755, loss_objectness=0.05248307349393144, loss_rpn_box_reg=0.1796951498836279\n",
      "Epoch 286/300: loss=2.2149233110249043, loss_classifier=0.8796072099357843, loss_objectness=0.052246260398533195, loss_rpn_box_reg=0.18169610993936658\n",
      "Epoch 287/300: loss=2.2141238898038864, loss_classifier=0.8872790150344372, loss_objectness=0.05398156568116974, loss_rpn_box_reg=0.18040224770084023\n",
      "Epoch 288/300: loss=2.1940142065286636, loss_classifier=0.8759180139750242, loss_objectness=0.0447219330817461, loss_rpn_box_reg=0.17976149660535157\n",
      "Epoch 289/300: loss=2.1830761544406414, loss_classifier=0.8755186479538679, loss_objectness=0.0373637615412008, loss_rpn_box_reg=0.18043964984826744\n",
      "Epoch 290/300: loss=2.2038851901888847, loss_classifier=0.8855811040848494, loss_objectness=0.049170209967996925, loss_rpn_box_reg=0.17981012491509318\n",
      "Epoch 291/300: loss=2.192056268453598, loss_classifier=0.8782581184059381, loss_objectness=0.04284434829605743, loss_rpn_box_reg=0.18082059361040592\n",
      "Epoch 292/300: loss=2.1958064436912537, loss_classifier=0.877101356163621, loss_objectness=0.050941110705025494, loss_rpn_box_reg=0.18132051778957248\n",
      "Epoch 293/300: loss=2.2088140211999416, loss_classifier=0.8855618014931679, loss_objectness=0.057331908552441746, loss_rpn_box_reg=0.18054579920135438\n",
      "Epoch 294/300: loss=2.191161572933197, loss_classifier=0.8800092870369554, loss_objectness=0.05036321806255728, loss_rpn_box_reg=0.18079986213706434\n",
      "Epoch 295/300: loss=2.181019365787506, loss_classifier=0.8719878494739532, loss_objectness=0.04556750738993287, loss_rpn_box_reg=0.17903063539415598\n",
      "Epoch 296/300: loss=2.2089564613997936, loss_classifier=0.8865458704531193, loss_objectness=0.05475294718053192, loss_rpn_box_reg=0.17938396660611033\n",
      "Epoch 297/300: loss=2.1808232311159372, loss_classifier=0.8778444407507777, loss_objectness=0.043517737911315635, loss_rpn_box_reg=0.17899692070204765\n",
      "Epoch 298/300: loss=2.1768710985779762, loss_classifier=0.8640405982732773, loss_objectness=0.04814430346596055, loss_rpn_box_reg=0.17910393164493144\n",
      "Epoch 299/300: loss=2.1769893802702427, loss_classifier=0.8748325295746326, loss_objectness=0.045977313217008486, loss_rpn_box_reg=0.17851786175742745\n",
      "Epoch 300/300: loss=2.191538792103529, loss_classifier=0.8785511227324605, loss_objectness=0.049724044161848724, loss_rpn_box_reg=0.18020845879800618\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "print(\"Start training ...\")\n",
    "\n",
    "for epoch in range(start_epoch+1, n_epochs+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_loss_classifier = 0\n",
    "    epoch_loss_objectness = 0\n",
    "    epoch_loss_rpn_box_reg = 0\n",
    "    for data in train_data_loader:\n",
    "        images = []\n",
    "        targets = []\n",
    "        for image, annotations in data:\n",
    "            images.append(image.to(device))\n",
    "            bboxes = list(map(lambda x: [x['bbox'][0], x['bbox'][1], x['bbox'][0]+x['bbox'][2], x['bbox'][1]+x['bbox'][3]], annotations))\n",
    "            labels = list(map(lambda x: int(x['category_id'])+1, annotations))\n",
    "            target = {}\n",
    "            target[\"boxes\"] = torch.tensor(bboxes).to(device)\n",
    "            target[\"labels\"] = torch.tensor(labels).to(device)\n",
    "            targets.append(target)\n",
    "        loss_dict = model(images, targets)\n",
    "        loss_classifier = loss_dict['loss_classifier'].detach().cpu().numpy()\n",
    "        loss_box_reg = loss_dict['loss_box_reg'].detach().cpu().numpy()\n",
    "        loss_objectness = loss_dict['loss_objectness'].detach().cpu().numpy()\n",
    "        loss_rpn_box_reg = loss_dict['loss_rpn_box_reg'].detach().cpu().numpy()\n",
    "        loss = sum(v for v in loss_dict.values())\n",
    "        loss_total = loss.detach().cpu().numpy()\n",
    "        epoch_loss += loss_total\n",
    "        epoch_loss_classifier += loss_classifier\n",
    "        epoch_loss_objectness += loss_objectness\n",
    "        epoch_loss_rpn_box_reg += loss_rpn_box_reg\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f\"Epoch {epoch}/{n_epochs}: loss={epoch_loss}, loss_classifier={epoch_loss_classifier}, loss_objectness={epoch_loss_objectness}, loss_rpn_box_reg={epoch_loss_rpn_box_reg}\")\n",
    "    if epoch > 0 and epoch % save_every == 0:\n",
    "        torch.save(model.state_dict(), get_weight_filename(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d56f633-c763-4e5c-b698-f17d373bd6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c61922-859a-43e0-8169-baa7aef021c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1], device='cuda:0') 1\n",
      "Pred: tensor([[ 18.7619, 128.9899,  40.0910, 145.1640],\n",
      "        [ 33.0134, 195.1446,  49.3381, 210.1378],\n",
      "        [511.8869, 215.2784, 511.9984, 227.7759]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "True: [[19, 129, 21, 16]]\n",
      "tensor([1, 1, 1], device='cuda:0') 1\n",
      "Pred: tensor([[ 16.1982, 126.9820,  45.7223, 149.0213],\n",
      "        [303.9715,   4.0852, 338.2688,  27.5187],\n",
      "        [253.2170,   6.8253, 289.8279,  31.9003]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "True: [[16, 127, 30, 22]]\n",
      "tensor([1, 1, 1], device='cuda:0') 3\n",
      "Pred: tensor([[301.9377,   3.0549, 338.2368,  28.9639],\n",
      "        [254.8243,   5.8796, 292.1436,  34.9454],\n",
      "        [279.0653,  52.9723, 293.8815,  74.1070]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "True: [[255, 6, 37, 29], [279, 53, 15, 21], [302, 3, 36, 26]]\n",
      "tensor([1, 1, 1, 1, 1, 1], device='cuda:0') 6\n",
      "Pred: tensor([[301.9292,   3.2488, 338.2710,  28.7269],\n",
      "        [254.9377,   6.0796, 292.0716,  35.0114],\n",
      "        [279.1774,  53.1440, 294.1367,  73.8520],\n",
      "        [459.9851,  88.8204, 474.8003, 106.0316],\n",
      "        [488.8483,  69.9720, 502.1979,  87.9796],\n",
      "        [428.8857, 105.8169, 442.0819, 131.2537]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "True: [[302, 3, 36, 26], [255, 6, 37, 29], [279, 53, 15, 21], [429, 106, 13, 25], [460, 89, 15, 17], [489, 70, 13, 18]]\n",
      "tensor([1, 1, 1, 1, 1, 1], device='cuda:0') 6\n",
      "Pred: tensor([[301.9976,   3.2629, 338.0393,  28.7509],\n",
      "        [255.1079,   6.2947, 292.0771,  34.9617],\n",
      "        [458.1532,  89.9855, 473.0570, 103.1828],\n",
      "        [419.8504, 109.2547, 440.9814, 130.9618],\n",
      "        [487.8088,  70.7581, 499.0627,  85.0107],\n",
      "        [278.9563,  52.9701, 293.8877,  74.0706]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "True: [[302, 3, 36, 26], [255, 6, 37, 29], [279, 53, 15, 21], [420, 109, 21, 22], [458, 90, 15, 13], [488, 71, 11, 14]]\n",
      "tensor([1, 1, 1, 1], device='cuda:0') 4\n",
      "Pred: tensor([[301.9932,   2.9658, 338.0704,  28.8205],\n",
      "        [418.0525, 109.8810, 438.0086, 129.0095],\n",
      "        [254.9094,   6.0156, 291.8836,  35.4522],\n",
      "        [278.8631,  52.8931, 294.0520,  73.9197]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "True: [[255, 6, 37, 29], [279, 53, 15, 21], [302, 3, 36, 26], [418, 110, 20, 19]]\n",
      "tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0') 5\n",
      "Pred: tensor([[301.8825,   3.1449, 337.7433,  28.7941],\n",
      "        [276.7902,  47.8406, 296.0550,  71.0422],\n",
      "        [254.8621,   5.9740, 291.9272,  35.2127],\n",
      "        [355.0645, 167.0512, 369.1132, 179.1002],\n",
      "        [294.1189, 153.0635, 302.0564, 168.9544],\n",
      "        [417.2799, 108.4858, 436.9666, 127.8872],\n",
      "        [379.9130, 176.9795, 394.8317, 190.7248]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "True: [[255, 6, 37, 29], [302, 3, 36, 26], [277, 48, 19, 23], [355, 167, 14, 12], [294, 153, 8, 16]]\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') 9\n",
      "Pred: tensor([[418.1130, 107.0362, 437.9669, 130.0042],\n",
      "        [260.0080, 299.9424, 278.0772, 321.1262],\n",
      "        [255.0672,   6.0889, 291.7054,  35.0915],\n",
      "        [301.7186,   3.0215, 338.1526,  28.7627],\n",
      "        [294.0976, 148.8710, 306.1535, 166.0948],\n",
      "        [275.9818,  51.0338, 294.0621,  71.9686],\n",
      "        [378.9463, 174.0243, 397.0932, 191.9388],\n",
      "        [334.9793, 184.0452, 348.1041, 198.0633],\n",
      "        [356.0000, 166.9058, 366.9759, 176.9536],\n",
      "        [226.3437, 364.3592, 242.5345, 380.6747]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "True: [[260, 300, 18, 21], [302, 3, 36, 26], [255, 6, 37, 29], [418, 107, 20, 23], [276, 51, 18, 21], [356, 167, 11, 10], [379, 174, 18, 18], [335, 184, 13, 14], [294, 149, 12, 17]]\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') 9\n",
      "Pred: tensor([[259.9742, 299.9798, 278.0679, 320.9388],\n",
      "        [254.8949,   6.0699, 292.1980,  35.0278],\n",
      "        [225.9518, 364.9940, 243.0938, 381.1368],\n",
      "        [231.9645, 321.0146, 243.9672, 336.0640],\n",
      "        [301.7573,   2.8512, 337.9555,  28.6029],\n",
      "        [422.8949, 109.9658, 436.8728, 128.8945],\n",
      "        [326.9284, 185.0614, 347.9435, 199.0061],\n",
      "        [379.0847, 177.0540, 391.9554, 195.8750],\n",
      "        [248.0568, 349.9614, 260.2882, 366.0728],\n",
      "        [278.0343,  52.8128, 292.5422,  72.0318]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "True: [[226, 365, 17, 16], [260, 300, 18, 21], [232, 321, 12, 15], [255, 6, 37, 29], [302, 3, 36, 26], [327, 185, 21, 14], [379, 177, 13, 19], [423, 110, 14, 19], [248, 350, 12, 16]]\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') 8\n",
      "Pred: tensor([[259.8858, 299.9368, 277.9606, 320.8664],\n",
      "        [225.9653, 364.9471, 242.9801, 381.0137],\n",
      "        [302.0120,   2.9471, 337.6729,  28.6813],\n",
      "        [265.9536, 262.9526, 283.9218, 280.0484],\n",
      "        [231.9750, 320.8381, 243.9592, 336.0916],\n",
      "        [250.8076,   4.2200, 292.3732,  31.8442],\n",
      "        [330.0398, 186.0376, 343.0509, 201.9773],\n",
      "        [247.9691, 348.1433, 259.0115, 362.0318]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "True: [[266, 263, 18, 17], [302, 3, 36, 26], [232, 321, 12, 15], [226, 365, 17, 16], [260, 300, 18, 21], [330, 186, 13, 16], [251, 4, 41, 28], [248, 348, 11, 14]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    images, annotations = train_dataset[i]\n",
    "    print(model([images.to(device)])[0]['labels'], len(annotations))\n",
    "    print(\"Pred:\", model([images.to(device)])[0]['boxes'])\n",
    "    print(\"True:\", list(map(lambda x: x['bbox'], annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6afbd-4fdf-430d-b1d5-bebff76303ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
